{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff1ffa1",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Neural Network Experiments with PyTorch\n",
    "\n",
    "This notebook demonstrates three different neural network architectures using PyTorch. We will generate a synthetic classification dataset and train three models of increasing complexity:\n",
    "\n",
    "1. **Simple neural network** – a single hidden layer.\n",
    "2. **Moderate network** – multiple hidden layers and dropout for regularization.\n",
    "3. **Advanced network** – deeper architecture with more neurons and batch normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894562de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (42703, 15)\n",
      "Training samples: 34162, Test samples: 8541\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the same dataset used in the provided Machine_Learning notebook\n",
    "# This file should exist relative to this notebook.\n",
    "DATA_PATH = 'data/matches_ml_prematch_clean.csv'\n",
    "\n",
    "# Read the CSV into a DataFrame\n",
    "# If the file is missing, this will raise an exception. Ensure the dataset is present.\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f'Loaded dataset: {df.shape}')\n",
    "\n",
    "# Define the target and features. The target column is named 'label'.\n",
    "# Drop any identifier columns such as 'match_id' if present.\n",
    "y = df['label'].astype(int)\n",
    "X = df.drop(columns=['label'])\n",
    "if 'match_id' in X.columns:\n",
    "    X = X.drop(columns=['match_id'])\n",
    "\n",
    "# Identify categorical columns (dtype == object or category) and encode them using one‑hot encoding.\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "if len(cat_cols) > 0:\n",
    "    X = pd.get_dummies(X, columns=cat_cols)\n",
    "\n",
    "# Standardize numerical features for better neural network training\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}, Test samples: {len(test_dataset)}')\n",
    "\n",
    "# Store input dimension for model summary\n",
    "INPUT_DIM = X_train_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6493f06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install torchinfo for model summaries (if not already installed)\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install torchinfo --quiet\n",
    "    from torchinfo import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b0332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=20):\n",
    "    model.train()\n",
    "    model.to(device)  # make sure model is on GPU\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # move batch to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader):.4f} | Accuracy: {accuracy:.4f}\")\n",
    "    return total_loss / len(test_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157d649",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Simple Neural Network\n",
    "\n",
    "Our baseline model consists of a single hidden layer with 64 neurons. It uses ReLU activation and is trained with cross‑entropy loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7854a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "SimpleNN                                 [1, 2]                    --\n",
      "├─Linear: 1-1                            [1, 64]                   1,728\n",
      "├─ReLU: 1-2                              [1, 64]                   --\n",
      "├─Linear: 1-3                            [1, 2]                    130\n",
      "==========================================================================================\n",
      "Total params: 1,858\n",
      "Trainable params: 1,858\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n",
      "Training Simple Neural Network...\n",
      "Epoch [1/20] - Loss: 0.2163\n",
      "Epoch [2/20] - Loss: 0.1766\n",
      "Epoch [3/20] - Loss: 0.1725\n",
      "Epoch [4/20] - Loss: 0.1707\n",
      "Epoch [5/20] - Loss: 0.1693\n",
      "Epoch [6/20] - Loss: 0.1682\n",
      "Epoch [7/20] - Loss: 0.1670\n",
      "Epoch [8/20] - Loss: 0.1666\n",
      "Epoch [9/20] - Loss: 0.1663\n",
      "Epoch [10/20] - Loss: 0.1656\n",
      "Epoch [11/20] - Loss: 0.1652\n",
      "Epoch [12/20] - Loss: 0.1648\n",
      "Epoch [13/20] - Loss: 0.1642\n",
      "Epoch [14/20] - Loss: 0.1640\n",
      "Epoch [15/20] - Loss: 0.1636\n",
      "Epoch [16/20] - Loss: 0.1632\n",
      "Epoch [17/20] - Loss: 0.1631\n",
      "Epoch [18/20] - Loss: 0.1625\n",
      "Epoch [19/20] - Loss: 0.1624\n",
      "Epoch [20/20] - Loss: 0.1620\n",
      "Test Loss: 0.1745 | Accuracy: 0.9201\n",
      "Simple NN Test Loss: 0.1745, Test Accuracy: 0.9201\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "simple_model = SimpleNN(input_dim=INPUT_DIM, hidden_dim=64, output_dim=2).to(device)\n",
    "\n",
    "\n",
    "# Show model summary\n",
    "print(summary(simple_model, input_size=(1, INPUT_DIM)))\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Simple Neural Network...\")\n",
    "simple_model = train_model(simple_model, train_loader, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "# Evaluate the model\n",
    "simple_test_loss, simple_test_acc = evaluate_model(simple_model, test_loader, criterion)\n",
    "\n",
    "print(f\"Simple NN Test Loss: {simple_test_loss:.4f}, Test Accuracy: {simple_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb338d",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Moderate Neural Network\n",
    "\n",
    "This model increases complexity by stacking three hidden layers (128→64→32 neurons) and applying dropout for regularization. Such architectures can capture more complex relationships in the data while mitigating overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff7b3761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ModerateNN                               [1, 2]                    --\n",
      "├─Sequential: 1-1                        [1, 2]                    --\n",
      "│    └─Linear: 2-1                       [1, 128]                  3,456\n",
      "│    └─ReLU: 2-2                         [1, 128]                  --\n",
      "│    └─Dropout: 2-3                      [1, 128]                  --\n",
      "│    └─Linear: 2-4                       [1, 64]                   8,256\n",
      "│    └─ReLU: 2-5                         [1, 64]                   --\n",
      "│    └─Dropout: 2-6                      [1, 64]                   --\n",
      "│    └─Linear: 2-7                       [1, 32]                   2,080\n",
      "│    └─ReLU: 2-8                         [1, 32]                   --\n",
      "│    └─Linear: 2-9                       [1, 2]                    66\n",
      "==========================================================================================\n",
      "Total params: 13,858\n",
      "Trainable params: 13,858\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.06\n",
      "Estimated Total Size (MB): 0.06\n",
      "==========================================================================================\n",
      "Training Moderate Neural Network...\n",
      "Epoch [1/25] - Loss: 0.2235\n",
      "Epoch [2/25] - Loss: 0.1792\n",
      "Epoch [3/25] - Loss: 0.1748\n",
      "Epoch [4/25] - Loss: 0.1732\n",
      "Epoch [5/25] - Loss: 0.1711\n",
      "Epoch [6/25] - Loss: 0.1696\n",
      "Epoch [7/25] - Loss: 0.1689\n",
      "Epoch [8/25] - Loss: 0.1669\n",
      "Epoch [9/25] - Loss: 0.1669\n",
      "Epoch [10/25] - Loss: 0.1661\n",
      "Epoch [11/25] - Loss: 0.1649\n",
      "Epoch [12/25] - Loss: 0.1652\n",
      "Epoch [13/25] - Loss: 0.1650\n",
      "Epoch [14/25] - Loss: 0.1637\n",
      "Epoch [15/25] - Loss: 0.1641\n",
      "Epoch [16/25] - Loss: 0.1650\n",
      "Epoch [17/25] - Loss: 0.1634\n",
      "Epoch [18/25] - Loss: 0.1627\n",
      "Epoch [19/25] - Loss: 0.1627\n",
      "Epoch [20/25] - Loss: 0.1632\n",
      "Epoch [21/25] - Loss: 0.1622\n",
      "Epoch [22/25] - Loss: 0.1624\n",
      "Epoch [23/25] - Loss: 0.1626\n",
      "Epoch [24/25] - Loss: 0.1614\n",
      "Epoch [25/25] - Loss: 0.1619\n",
      "Test Loss: 0.1695 | Accuracy: 0.9194\n",
      "Moderate NN Test Loss: 0.1695, Test Accuracy: 0.9194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ModerateNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ModerateNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate model\n",
    "moderate_model = ModerateNN(input_dim=INPUT_DIM, output_dim=2).to(device)\n",
    "\n",
    "# Show model summary\n",
    "print(summary(moderate_model, input_size=(1, INPUT_DIM)))\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(moderate_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Moderate Neural Network...\")\n",
    "moderate_model = train_model(moderate_model, train_loader, criterion, optimizer, num_epochs=25)\n",
    "\n",
    "# Evaluate the model\n",
    "moderate_test_loss, moderate_test_acc = evaluate_model(moderate_model, test_loader, criterion)\n",
    "\n",
    "print(f\"Moderate NN Test Loss: {moderate_test_loss:.4f}, Test Accuracy: {moderate_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54345bcd",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Advanced Deep Neural Network\n",
    "\n",
    "The final model pushes complexity further with five hidden layers (256→128→64→64→32 neurons), batch normalization, and dropout. Deeper networks can model highly non‑linear decision boundaries and are more expressive, though they may require careful tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae558ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AdvancedNN                               [1, 2]                    --\n",
      "├─Sequential: 1-1                        [1, 2]                    --\n",
      "│    └─Linear: 2-1                       [1, 256]                  6,912\n",
      "│    └─BatchNorm1d: 2-2                  [1, 256]                  512\n",
      "│    └─ReLU: 2-3                         [1, 256]                  --\n",
      "│    └─Dropout: 2-4                      [1, 256]                  --\n",
      "│    └─Linear: 2-5                       [1, 128]                  32,896\n",
      "│    └─BatchNorm1d: 2-6                  [1, 128]                  256\n",
      "│    └─ReLU: 2-7                         [1, 128]                  --\n",
      "│    └─Dropout: 2-8                      [1, 128]                  --\n",
      "│    └─Linear: 2-9                       [1, 64]                   8,256\n",
      "│    └─BatchNorm1d: 2-10                 [1, 64]                   128\n",
      "│    └─ReLU: 2-11                        [1, 64]                   --\n",
      "│    └─Dropout: 2-12                     [1, 64]                   --\n",
      "│    └─Linear: 2-13                      [1, 64]                   4,160\n",
      "│    └─ReLU: 2-14                        [1, 64]                   --\n",
      "│    └─Linear: 2-15                      [1, 32]                   2,080\n",
      "│    └─ReLU: 2-16                        [1, 32]                   --\n",
      "│    └─Linear: 2-17                      [1, 2]                    66\n",
      "==========================================================================================\n",
      "Total params: 55,266\n",
      "Trainable params: 55,266\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 0.06\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.22\n",
      "Estimated Total Size (MB): 0.23\n",
      "==========================================================================================\n",
      "Training Advanced Neural Network...\n",
      "Epoch [1/30] - Loss: 0.2643\n",
      "Epoch [2/30] - Loss: 0.2297\n",
      "Epoch [3/30] - Loss: 0.2199\n",
      "Epoch [4/30] - Loss: 0.2182\n",
      "Epoch [5/30] - Loss: 0.2155\n",
      "Epoch [6/30] - Loss: 0.2143\n",
      "Epoch [7/30] - Loss: 0.2098\n",
      "Epoch [8/30] - Loss: 0.2085\n",
      "Epoch [9/30] - Loss: 0.2046\n",
      "Epoch [10/30] - Loss: 0.2042\n",
      "Epoch [11/30] - Loss: 0.2052\n",
      "Epoch [12/30] - Loss: 0.2038\n",
      "Epoch [13/30] - Loss: 0.1996\n",
      "Epoch [14/30] - Loss: 0.1979\n",
      "Epoch [15/30] - Loss: 0.1966\n",
      "Epoch [16/30] - Loss: 0.1960\n",
      "Epoch [17/30] - Loss: 0.1920\n",
      "Epoch [18/30] - Loss: 0.1928\n",
      "Epoch [19/30] - Loss: 0.1891\n",
      "Epoch [20/30] - Loss: 0.1898\n",
      "Epoch [21/30] - Loss: 0.1898\n",
      "Epoch [22/30] - Loss: 0.1872\n",
      "Epoch [23/30] - Loss: 0.1872\n",
      "Epoch [24/30] - Loss: 0.1865\n",
      "Epoch [25/30] - Loss: 0.1875\n",
      "Epoch [26/30] - Loss: 0.1831\n",
      "Epoch [27/30] - Loss: 0.1846\n",
      "Epoch [28/30] - Loss: 0.1816\n",
      "Epoch [29/30] - Loss: 0.1824\n",
      "Epoch [30/30] - Loss: 0.1812\n",
      "Test Loss: 0.1697 | Accuracy: 0.9204\n",
      "Advanced NN Test Loss: 0.1697, Test Accuracy: 0.9204\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AdvancedNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AdvancedNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Instantiate model\n",
    "advanced_model = AdvancedNN(input_dim=INPUT_DIM, output_dim=2).to(device)\n",
    "\n",
    "# Show model summary\n",
    "print(summary(advanced_model, input_size=(1, INPUT_DIM)))\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(advanced_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Advanced Neural Network...\")\n",
    "advanced_model = train_model(advanced_model, train_loader, criterion, optimizer, num_epochs=30)\n",
    "\n",
    "# Evaluate the model\n",
    "advanced_test_loss, advanced_test_acc = evaluate_model(advanced_model, test_loader, criterion)\n",
    "\n",
    "print(f\"Advanced NN Test Loss: {advanced_test_loss:.4f}, Test Accuracy: {advanced_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66797f",
   "metadata": {},
   "source": [
    "\n",
    "## Results Summary\n",
    "\n",
    "After training the three architectures, we compare their test performance:\n",
    "\n",
    "| Model            | Test Loss | Test Accuracy |\n",
    "|------------------|----------:|--------------:|\n",
    "| **Simple NN**    | {{simple_loss}} | {{simple_acc}} |\n",
    "| **Moderate NN**  | {{moderate_loss}} | {{moderate_acc}} |\n",
    "| **Advanced NN**  | {{advanced_test_loss}} | {{advanced_acc}} |\n",
    "\n",
    "Even though the advanced model is deeper and uses normalization and dropout, its performance may not always dramatically surpass simpler networks on small or synthetic datasets. When selecting a model, consider the complexity of your problem, available computational resources, and the risk of overfitting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e524deb-35d0-432e-9103-4aa200a1f544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 18,961,410\n",
      "Training Extremely Deep Residual MLP...\n",
      "Epoch [1/20] - Loss: 0.2339\n",
      "Epoch [2/20] - Loss: 0.2013\n",
      "Epoch [3/20] - Loss: 0.2066\n",
      "Epoch [4/20] - Loss: 0.1926\n",
      "Epoch [5/20] - Loss: 0.1834\n",
      "Epoch [6/20] - Loss: 0.1770\n",
      "Epoch [7/20] - Loss: 0.1759\n",
      "Epoch [8/20] - Loss: 0.1746\n",
      "Epoch [9/20] - Loss: 0.1734\n",
      "Epoch [10/20] - Loss: 0.1819\n",
      "Epoch [11/20] - Loss: 0.1924\n",
      "Epoch [12/20] - Loss: 0.1826\n",
      "Epoch [13/20] - Loss: 0.1744\n",
      "Epoch [14/20] - Loss: 0.1711\n",
      "Epoch [15/20] - Loss: 0.1693\n",
      "Epoch [16/20] - Loss: 0.1691\n",
      "Epoch [17/20] - Loss: 0.1688\n",
      "Epoch [18/20] - Loss: 0.1683\n",
      "Epoch [19/20] - Loss: 0.1674\n",
      "Epoch [20/20] - Loss: 0.1665\n",
      "Evaluating...\n",
      "Test Loss: 0.1739 | Accuracy: 0.9173\n",
      "Deep Residual MLP — Test Loss: 0.1739, Accuracy: 0.9173\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------- Residual MLP Blocks --------\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    PreNorm Residual block for tabular MLPs:\n",
    "      x -> LN/BN -> Linear(H,H) -> GELU -> Dropout -> Linear(H,H) -> Dropout -> + x\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim: int, dropout: float = 0.2, norm: str = \"layernorm\"):\n",
    "        super().__init__()\n",
    "        if norm.lower() == \"batchnorm\":\n",
    "            self.norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "            self.norm2 = nn.BatchNorm1d(hidden_dim)\n",
    "        else:\n",
    "            self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "            self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.act = nn.GELU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm residual (works well for deep nets)\n",
    "        if isinstance(self.norm1, nn.BatchNorm1d):\n",
    "            y = self.norm1(x)  # BN expects (N, C)\n",
    "        else:\n",
    "            y = self.norm1(x)\n",
    "        y = self.fc1(y)\n",
    "        y = self.act(y)\n",
    "        y = self.drop(y)\n",
    "        if isinstance(self.norm2, nn.BatchNorm1d):\n",
    "            y = self.norm2(y)\n",
    "        else:\n",
    "            y = self.norm2(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.drop(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class ExtremelyDeepResidualMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Extremely deep residual MLP for tabular data.\n",
    "    Uses: Stem -> [ResidualBlock x N] -> Head\n",
    "    Target: up to ~20M parameters (controlled via hidden_dim & num_blocks).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        output_dim: int = 2,\n",
    "        hidden_dim: int = 1024,     # 1024 is a good sweet spot for ~20M\n",
    "        num_blocks: int = 9,        # 9 blocks @1024 ≈ ~18.9M params total\n",
    "        dropout: float = 0.2,\n",
    "        norm: str = \"layernorm\"     # \"layernorm\" or \"batchnorm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        blocks = [ResidualBlock(hidden_dim, dropout=dropout, norm=norm) for _ in range(num_blocks)]\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim) if norm.lower() != \"batchnorm\" else nn.Identity(),\n",
    "            nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "        )\n",
    "\n",
    "        # Good practice for stability with deep/wide MLPs\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)  # small init helps deep nets\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------- Helper: parameter count --------\n",
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# -------- Instantiate model (GPU ready) --------\n",
    "# Assumes you already defined:\n",
    "#   INPUT_DIM = X_train.shape[1]\n",
    "#   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "deep_res_model = ExtremelyDeepResidualMLP(\n",
    "    input_dim=INPUT_DIM,\n",
    "    output_dim=2,         # change if needed\n",
    "    hidden_dim=1024,      # tune this with num_blocks to control param count\n",
    "    num_blocks=9,         # 9 @ 1024 ≈ ~18.9M params\n",
    "    dropout=0.2,\n",
    "    norm=\"layernorm\"      # try \"batchnorm\" as well; LN is often simpler for tabular\n",
    ").to(device)\n",
    "\n",
    "print(\"Trainable parameters:\", f\"{count_parameters(deep_res_model):,}\")\n",
    "\n",
    "# Optional: detailed summary (requires torchinfo)\n",
    "try:\n",
    "    from torchinfo import summary\n",
    "    summary(deep_res_model, input_size=(1, INPUT_DIM))\n",
    "except Exception as e:\n",
    "    print(\"torchinfo summary unavailable:\", e)\n",
    "\n",
    "# Example: training setup (reuse your loaders & training/eval funcs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(deep_res_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "print(\"Training Extremely Deep Residual MLP...\")\n",
    "deep_res_model = train_model(deep_res_model, train_loader, criterion, optimizer, num_epochs=20)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "deep_test_loss, deep_test_acc = evaluate_model(deep_res_model, test_loader, criterion)\n",
    "print(f\"Deep Residual MLP — Test Loss: {deep_test_loss:.4f}, Accuracy: {deep_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79634d17-d6c5-4d26-90ad-e2327c4455de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
